{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/commons/home/jsingh/.conda/envs/spatial/lib/python3.9/site-packages/dask_jobqueue/core.py:20: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import tmpfile\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import mode\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from scipy.optimize import least_squares\n",
    "import time\n",
    "import dask.array as da\n",
    "from dask import delayed\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client, wait\n",
    "from dask_image.ndfilters import gaussian_filter\n",
    "from dask_image.ndmeasure import label\n",
    "from dask_image.ndmorph import binary_closing\n",
    "from skimage.registration import phase_cross_correlation\n",
    "from skimage.util import apply_parallel\n",
    "from skimage.morphology import local_maxima, disk\n",
    "from skimage.segmentation import watershed\n",
    "from math import floor\n",
    "from os.path import exists, join\n",
    "from os import makedirs, getcwd\n",
    "from pyseq import image_analysis as ia\n",
    "\n",
    "\n",
    "xr.set_options(keep_attrs=True)\n",
    "\n",
    "\n",
    "#dask.config.set({'temporary-directory': '/scratch'})\n",
    "def get_cluster(queue_name = 'pe2', log_dir=None):\n",
    "    \"\"\" Make dask cluster w/ workers = 2 cores, 32 G mem, and 1 hr wall time.\n",
    "\n",
    "        return cluster, client\n",
    "    \"\"\"\n",
    "\n",
    "    if log_dir is None:\n",
    "        log_dir = join(getcwd(),'dask_logs')\n",
    "        makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    cluster = SLURMCluster(\n",
    "                queue = queue_name,\n",
    "                cores = 2,\n",
    "                memory = '32G',\n",
    "                walltime='1:00:00',\n",
    "                log_directory=log_dir,\n",
    "                extra=[\"--lifetime\", \"55m\", \"--lifetime-stagger\", \"4m\"])\n",
    "    client = Client(cluster, scheduler = 'multiprocessing')\n",
    "\n",
    "    return cluster, client\n",
    "\n",
    "def q_bin(X, downscale=2):\n",
    "    \"\"\"Bin and quantize X (2D XArray), int16 quantization limit.\n",
    "\n",
    "        Parameters:\n",
    "        downscale (int): Factor to downscale both dimensions of X by.\n",
    "\n",
    "        Returns:\n",
    "        (dask array): Quantized and binned dask array, dtype=int16\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = da.coarsen(np.mean,X.data,{0:downscale,1:downscale},trim_excess=True,dtype='int16')\n",
    "\n",
    "    return X\n",
    "\n",
    "def mutual_information(X, Y):\n",
    "    \"\"\"Compute mutual information between X and Y dask arrays.\"\"\"\n",
    "\n",
    "\n",
    "    # compute limits\n",
    "    X = X.compute()\n",
    "    xmin = da.min(X).compute()\n",
    "    xmax = da.max(X).compute()\n",
    "\n",
    "    Y = Y.compute()\n",
    "    ymin = da.min(Y).compute()\n",
    "    ymax = da.max(Y).compute()\n",
    "\n",
    "    # compute indices\n",
    "    xi = X-xmin\n",
    "    yi = Y-ymin\n",
    "\n",
    "    # reshape for histogram\n",
    "    XY = da.rechunk(da.stack([da.ravel(X),da.ravel(Y)],axis=1),chunks=(1e6,-1), balance=True)\n",
    "\n",
    "    # joint probability distribution\n",
    "    xbins = range(xmin, xmax+2)\n",
    "    ybins = range(ymin, ymax+2)\n",
    "    H, edges = da.histogramdd(XY, bins=[xbins, ybins], density = True)\n",
    "    H = H.compute()\n",
    "\n",
    "    # x marginal probability distribution\n",
    "    p_x = H.sum(axis=1)\n",
    "    # y marginal probability distribution\n",
    "    p_y = H.sum(axis=0)\n",
    "\n",
    "\n",
    "    H_ = da.from_array(H[xi,yi])\n",
    "    x_ = da.from_array(p_x[xi])\n",
    "    y_ = da.from_array(p_y[yi])\n",
    "\n",
    "    # Mutual information I(X,Y)\n",
    "    try:\n",
    "        MI=da.sum(H_*da.log(H_/(x_*y_)))\n",
    "        MI=MI.compute()\n",
    "    except:\n",
    "        print('H', H.shape)\n",
    "        print('px', p_x.shape)\n",
    "        print('py', p_y.shape)\n",
    "        print('MI failed',xbins, ybins)\n",
    "        MI = None\n",
    "\n",
    "    return MI\n",
    "\n",
    "def picasso(im_chi, im_chj, alpha_min=0, alpha_max=2, n_alpha=20, alpha_path=None, downscale=2,\n",
    "            fit_alpha=True, **kwargs):\n",
    "    \"\"\"Estimate mixing parameter to minimize mutual information between images.\n",
    "\n",
    "    Parameters:\n",
    "    im_chi (Xarray): Image to remove crosstalk from.\n",
    "    im_chj (Xarray): Image that is source of crosstalk.\n",
    "    alpha_min (int/float): minimum mixing parameter to consider\n",
    "    alpha_max (int/float): maximum mixing parameter to consider\n",
    "    n_alpha (int): Number of mixing paramters between alpha_min and alpha_max\n",
    "    alpha_path (path): File to save computed mutual information for each mixing parameter\n",
    "    downscale (int): Factor to downscale images by mutual information calculation\n",
    "\n",
    "    Returns:\n",
    "    (float): Estimate of optimal mixing parameter\n",
    "\n",
    "    \"\"\"\n",
    "    # im_chi = image from channel i\n",
    "    # im_chj = image from channel j\n",
    "\n",
    "    assert len(im_chi.shape) == 2\n",
    "    assert len(im_chj.shape) == 2\n",
    "\n",
    "    Y = q_bin(im_chj, downscale)\n",
    "\n",
    "    print('Computing mutual information across range of alphas')\n",
    "\n",
    "    if alpha_path is not None:\n",
    "        if exists(alpha_path):\n",
    "            alpha = np.loadtxt(alpha_path)\n",
    "        else:\n",
    "            alpha_path = None\n",
    "\n",
    "    if alpha_path is None:\n",
    "        alpha_path = make_alpha_path(im_chi,im_chj)\n",
    "        alpha = np.linspace(alpha_min,alpha_max,n_alpha)\n",
    "        alpha = np.vstack([alpha,np.zeros_like(alpha)]).T\n",
    "        np.savetxt(alpha_path, alpha)\n",
    "\n",
    "    for i, (a, mi) in enumerate(alpha):\n",
    "        if mi == 0:\n",
    "            X = q_bin(im_chi-a*im_chj, downscale)\n",
    "            mi = mutual_information(X,Y)\n",
    "            alpha[i,1] = mi\n",
    "            np.savetxt(alpha_path, alpha)\n",
    "            print('PICASSO::'+alpha_path[0:-4]+'::',a,mi)\n",
    "\n",
    "    if fit_alpha:\n",
    "        fit = interp1d(alpha[:,0], alpha[:,1], kind='cubic')\n",
    "        fine_alpha = np.linspace(alpha_min,alpha_max,n_alpha*10)\n",
    "        opt_alpha = fine_alpha[np.argmin(fit(fine_alpha))]\n",
    "    else:\n",
    "        opt_alpha = -1\n",
    "\n",
    "    return opt_alpha\n",
    "\n",
    "def gaussian(x, *args):\n",
    "    \"\"\"Gaussian function for curve fitting that can do multiple peaks.\n",
    "\n",
    "        Parameters:\n",
    "        x (array): array of x values\n",
    "        args (list): model parameters with length 3*n_peaks\n",
    "              [amp0, amp1, ...ampN, cen0, cen1, ...cennN, sig0, sig1, ...sigN]\n",
    "\n",
    "        Returns:\n",
    "        (array): result of model with parameters computed with x\n",
    "    \"\"\"\n",
    "\n",
    "    if len(args) == 1:\n",
    "      args = args[0]\n",
    "\n",
    "    n_peaks = int(len(args)/3)\n",
    "\n",
    "\n",
    "    if len(args) - n_peaks*3 != 0:\n",
    "      print('Unequal number of parameters')\n",
    "    else:\n",
    "      for i in range(n_peaks):\n",
    "        amp = args[0:n_peaks]\n",
    "        cen = args[n_peaks:n_peaks*2]\n",
    "        sigma = args[n_peaks*2:n_peaks*3]\n",
    "\n",
    "      g_sum = 0\n",
    "      for i in range(len(amp)):\n",
    "          g_sum += amp[i]*(1/(sigma[i]*(np.sqrt(2*np.pi))))*(np.exp((-1.0/2.0)*(((x-cen[i])/sigma[i])**2)))\n",
    "\n",
    "      return g_sum\n",
    "\n",
    "def res_gaussian(args, xfun, yfun):\n",
    "    \"\"\"Gaussian residual function for curve fitting.\"\"\"\n",
    "\n",
    "    g_sum = gaussian(xfun, args)\n",
    "\n",
    "    return yfun-g_sum\n",
    "\n",
    "def fit_mixed_gaussian(x, y, max_peaks=8, amplb=-np.inf, ampub=np.inf, tolerance=0.98, **kwargs):\n",
    "    \"\"\"Fit x vs y to a mixed gaussian model.\n",
    "\n",
    "        Parameters:\n",
    "        x (array): dependent variable\n",
    "        y (array): independt variable\n",
    "        max_peaks (int): maximum number of peaks to fit\n",
    "        amp_lb: lower bound of amplitude\n",
    "        amp_ub: upper bound of amplitude\n",
    "        tolerance (float): Add peaks to model until R^2 is above this threshold\n",
    "\n",
    "        Returns:\n",
    "        (list): Model parameters with length 3*n_peaks\n",
    "                [amp0, amp1, ...ampN, cen0, cen1, ...cennN, sig0, sig1, ...sigN]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # initialize values\n",
    "    peaks = None; R2 = None\n",
    "    # Initialize variables\n",
    "    amp = []; amp_lb = []; amp_ub = [];\n",
    "    cen = []; cen_lb = []; cen_ub = []; cen_guess = x[np.argmax(y)]\n",
    "    cenlb = np.min(x); cenub=np.max(x)\n",
    "    sigma = []; sigma_lb = []; sigma_ub = []; sigma_guess = np.sum(y**2)**0.5\n",
    "    SST = np.sum((y-np.mean(y))**2)\n",
    "    R2 = 0\n",
    "\n",
    "    # Add peaks until fit reaches threshold\n",
    "    while len(amp) <= max_peaks and R2 < tolerance:\n",
    "\n",
    "        # calculate initial guesses\n",
    "        if len(amp) == 0:\n",
    "            amp_guess = np.max(y)\n",
    "            cen_guess = x[np.argmax(y)]\n",
    "            sigma_guess = np.sum(y**2)**0.5\n",
    "        else:\n",
    "            ind = np.argmax(y-results.fun)\n",
    "            amp_guess = y[ind]\n",
    "            cen_guess = x[ind]\n",
    "            sigma_guess = np.sum(results.fun**2)**0.5\n",
    "\n",
    "        # set initial guesses\n",
    "        amp.append(amp_guess)\n",
    "        cen.append(cen_guess)\n",
    "        sigma.append(sigma_guess)\n",
    "        p0 = np.array([amp, cen, sigma])\n",
    "        p0 = p0.flatten()\n",
    "\n",
    "        # set bounds\n",
    "        amp_lb.append(amplb); amp_ub.append(ampub)\n",
    "        cen_lb.append(cenlb); cen_ub.append(cenub)\n",
    "        sigma_lb.append(0); sigma_ub.append(np.inf)\n",
    "        lo_bounds = np.array([amp_lb, cen_lb, sigma_lb])\n",
    "        up_bounds = np.array([amp_ub, cen_ub, sigma_ub])\n",
    "        lo_bounds = lo_bounds.flatten()\n",
    "        up_bounds = up_bounds.flatten()\n",
    "\n",
    "        # Optimize parameters\n",
    "        results = least_squares(res_gaussian, p0, bounds=(lo_bounds,up_bounds),\n",
    "                                args=(x,y))\n",
    "\n",
    "\n",
    "        if not results.success:\n",
    "            print(results.message)\n",
    "        else:\n",
    "            R2 = 1 - np.sum(results.fun**2)/SST\n",
    "            print('R2=',R2,'with',len(amp),'peaks')\n",
    "\n",
    "\n",
    "        if results.success and R2 > tolerance:\n",
    "            peaks = np.reshape(results.x,(-1,len(amp))).T\n",
    "            peaks = peaks[np.argsort(peaks[:,1])[::-1],:]\n",
    "            for i, row in enumerate(peaks):\n",
    "                print('peak',i,':: amplitude =', row[0],'center=',row[1],'sigma=',row[2])\n",
    "\n",
    "\n",
    "        else:\n",
    "            if len(amp) == max_peaks:\n",
    "                print('Bad fit')\n",
    "                break\n",
    "\n",
    "    return peaks, R2\n",
    "\n",
    "def make_alpha_path(im_chi, im_chj):\n",
    "    \"\"\"Return filename to save mixing parameter vs mutual information.\"\"\"\n",
    "\n",
    "    alpha_path = im_chi.name\n",
    "    chi = im_chi.channel.values\n",
    "    chj = im_chj.channel.values\n",
    "    cy = im_chi.cycle.values\n",
    "    alpha_path+= '_i'+str(chi)+'j'+str(chj)+'c'+str(cy)+'.txt'\n",
    "\n",
    "    return alpha_path\n",
    "\n",
    "\n",
    "def get_mixing_matrix(im, channel_pairs, Mpath=None, **kwargs):\n",
    "    \"\"\"Return mixing matrix to unmix channels.\n",
    "\n",
    "        Uses PICASSO algorithm to calculate mixing parameters between images as\n",
    "        defined in channel pairs.\n",
    "        chanels_pairs={cycle number:[(channel1, channel2), (channel3, channel4)]}\n",
    "\n",
    "        Parameters:\n",
    "        im (HiSeqImage): Images from a HiSeq\n",
    "        channel_pairs: dict of channels to unmix, keys are cycle numbers, values\n",
    "                       are channel pairs, see above\n",
    "        Mpath(path): Filename to save mixing matrix\n",
    "        downscale(int): Factor to downscale images during computatin of mixing matrix\n",
    "\n",
    "        Returns:\n",
    "        (array): ncycles x nchannels x nchannels mixing matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    im = one_z_plane(im)\n",
    "\n",
    "    cycles = list(im.cycle.values)\n",
    "    channels = list(im.channel.values)\n",
    "    ncy = len(cycles)\n",
    "    nch = len(channels)\n",
    "\n",
    "    if Mpath is None:\n",
    "        Mpath = im.name+'_MixingMatrix.txt'\n",
    "\n",
    "    if exists(Mpath):\n",
    "        M = np.loadtxt(Mpath)\n",
    "        M = M.reshape((ncy,nch,nch))\n",
    "    else:\n",
    "        M = np.zeros((ncy,nch,nch))\n",
    "        for i in range(ncy):\n",
    "            M[i,:,:] = np.identity(nch)\n",
    "        np.savetxt(Mpath,M.flatten())\n",
    "\n",
    "    for cyi, cy in enumerate(cycles):\n",
    "        # Get Channel Pairs\n",
    "        if type(channel_pairs) == dict:\n",
    "            ch_pairs = channel_pairs[cy]\n",
    "        else:\n",
    "            ch_pairs = channel_pairs\n",
    "\n",
    "        for chi, chj in ch_pairs:\n",
    "\n",
    "            i = channels.index(chi)\n",
    "            j = channels.index(chj)\n",
    "\n",
    "            if M[cyi,i,j] == 0:\n",
    "\n",
    "                # Open images\n",
    "                im_chi = im.sel(channel = chi, cycle = cy)\n",
    "                im_chj = im.sel(channel = chj, cycle = cy)\n",
    "                alpha_path = make_alpha_path(im_chi, im_chj)\n",
    "\n",
    "                # remove background\n",
    "                mini = im_chi.min(dim=['row','col']).astype('int16')\n",
    "                minj = im_chj.min(dim=['row','col']).astype('int16')\n",
    "                im_chi = im_chi - mini\n",
    "                im_chj = im_chj - minj\n",
    "\n",
    "                # Find alpha that minimizes mutual information\n",
    "                start = time.time()\n",
    "                try:\n",
    "                    alpha = picasso(im_chi, im_chj, alpha_path=alpha_path, **kwargs)\n",
    "                    stop = time.time()\n",
    "                    print('PICASSO::',im.name,'::',chi,chj,'::',alpha,(stop-start)/60)\n",
    "                    M[cyi,i,j] = alpha\n",
    "                    np.savetxt(Mpath,M.flatten())\n",
    "                except:\n",
    "                    print('PICASSO::',im.name,'::',chi,chj,'::FAILED')\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "def unmix(im, MM, update_size =1, DOT = True, client = None):\n",
    "    \"\"\"Unmix images using mixing matrix\n",
    "\n",
    "        Parameters:\n",
    "            im (Xarray): Images to unmix\n",
    "            MM (array): mixing matrix\n",
    "            update_size: Not in use potentially for PICASSO\n",
    "            DOT (bool): True to use matrix dot product, or\n",
    "                        False to compute channels seperately (for large images)\n",
    "            client (dask scheduler): Used with DOT = False\n",
    "\n",
    "        Returns:\n",
    "            (Xarray): unmixed image\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    im = im.sortby('channel')\n",
    "\n",
    "    ncycles, n_chi, n_chj = MM.shape\n",
    "    cycles = im.cycle.values\n",
    "    if 'obj_step' in im.dims:\n",
    "        zplanes = im.obj_step.values\n",
    "    else:\n",
    "        im = im.expand_dims('obj_step')\n",
    "        zplanes = im.obj_step.values\n",
    "    nch = len(im.channel.values)\n",
    "\n",
    "    if ncycles != len(cycles):\n",
    "        print('Image cycles and Mixing Matrix cycles do not match')\n",
    "        raise RuntimeError\n",
    "\n",
    "    if n_chi != nch or n_chj != nch:\n",
    "        print('Image channels and Mixing Matrix channels do not match')\n",
    "        raise RuntimeError\n",
    "\n",
    "    cycle_ims = []\n",
    "    for i, cy in enumerate(cycles):\n",
    "        print('Unmixing cycle', cy)\n",
    "        # make unmixing matrix\n",
    "        M = MM[i,:,:]\n",
    "        mask = ~np.eye(n_chi, dtype=bool)\n",
    "        M_unmix = np.identity(n_chi)\n",
    "        M_unmix[mask] = M[mask]*(-update_size)\n",
    "        dims = ['channeli', 'channel']\n",
    "        ch_coords = im.coords['channel'].values\n",
    "        coords={'channeli':ch_coords, 'channel':ch_coords}\n",
    "        M_unmix = xr.DataArray(M_unmix, dims=dims, coords=coords)\n",
    "\n",
    "        z_ims = []\n",
    "        for z in zplanes:\n",
    "            X = im.sel(cycle=cy, obj_step = z)\n",
    "\n",
    "            if DOT:\n",
    "                # Unmix X and and update dimension names\n",
    "                X_update = M_unmix.dot(X, dims='channel').swap_dims({'channeli':'channel'})\n",
    "                X_update = X_update.astype('int16')\n",
    "                X_update = X_update.where(X_update > 0, 0) #remove pixels less than 0\n",
    "                X_update = X_update.rename({'channeli':'channel'})\n",
    "                X_unmix = X_update.assign_coords({'channel':ch_coords})\n",
    "            else:\n",
    "                X_unmix = unmix_split_channel(M_unmix, X, client)\n",
    "\n",
    "            #Save unmixed image\n",
    "            z_ims.append(X_unmix)\n",
    "\n",
    "        # Stack z planes\n",
    "        zstack = xr.concat(z_ims,dim='obj_step')\n",
    "        #Save unmixed z stack\n",
    "        cycle_ims.append(zstack)\n",
    "\n",
    "    # Stack Cycles\n",
    "    unmix_im = xr.concat(cycle_ims,dim='cycle')\n",
    "    unmix_im.attrs = im.attrs\n",
    "    unmix_im.name = im.name\n",
    "\n",
    "    return unmix_im\n",
    "\n",
    "def unmix_split_channel(M_unmix, X, client=None):\n",
    "    \"\"\"Unmix channels seperately.\n",
    "\n",
    "        Parameters:\n",
    "        M_unmix (array): Unmixing matrix\n",
    "        X (3-d image): Channels x row x col images\n",
    "        client (dask scheduler): Schedular for cluster\n",
    "\n",
    "        Returns:\n",
    "        (xarray DataArray): Unmixed Channels x row x col images\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    channels = X.channel.values\n",
    "\n",
    "    ch_list = []\n",
    "    for chi in channels:\n",
    "        alphas = M_unmix.sel(channeli = chi)\n",
    "        alphas = alphas.reset_coords(names='channeli',drop=True)\n",
    "\n",
    "        if np.sum(alphas) == 1:\n",
    "            ch_list.append(X.sel(channel = chi))\n",
    "        else:\n",
    "            im = None\n",
    "            for a, chj in zip(alphas, channels):\n",
    "                if a != 0 and im is None:\n",
    "                    im = a*X.sel(channel = chj)\n",
    "                elif a !=0 and im is not None:\n",
    "                    im = im+a*(X.sel(channel = chj))\n",
    "            im = im.assign_coords({'channel':chi})\n",
    "            im = im.astype('int16')\n",
    "            im = im.where(im > 0, 0) #remove pixels less than 0\n",
    "            if client is not None:\n",
    "                im = client.persist(im, retries=10)\n",
    "                wait(im)\n",
    "\n",
    "\n",
    "            ch_list.append(im)\n",
    "\n",
    "    # Stack channels planes\n",
    "    chstack = xr.concat(ch_list,dim='channel')\n",
    "\n",
    "    return chstack\n",
    "\n",
    "\n",
    "def get_cycle_shift(im, ref_channel=610, ref_cycle=1):\n",
    "    \"\"\"Calculate shift to align images across cycles.\n",
    "\n",
    "        Uses phase cross correlation\n",
    "\n",
    "        Parameters:\n",
    "            im (Xarray): Images to align\n",
    "            ref_channel: Reference channel to align images to\n",
    "            ref_cycle: Refernce cycle to align images to\n",
    "\n",
    "        Returns:\n",
    "            (array): shape = ncycles x 2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rows = len(im.row)\n",
    "    cols = len(im.col)\n",
    "    cycles = im.cycle.values\n",
    "\n",
    "    im = one_z_plane(im)\n",
    "\n",
    "    # Crop if image is large\n",
    "    crop_size = 2048*3\n",
    "    if rows*cols > 2*crop_size**2:\n",
    "        center_row = int(rows/2); center_col = int(cols/2)\n",
    "        upper_row = int(center_row + crop_size/2); lower_row = int(center_row - crop_size/2)\n",
    "        left_col  = int(center_col - crop_size/2); right_col = int(center_col + crop_size/2)\n",
    "        bound_box = [lower_row, left_col,\n",
    "                     upper_row, left_col,\n",
    "                     upper_row, right_col]\n",
    "        bound_box = np.reshape(np.array(bound_box), (3,2))\n",
    "        im = ia.HiSeqImages(im = im)\n",
    "        im.crop_section(bound_box)\n",
    "        im = im.im\n",
    "\n",
    "\n",
    "    im_ref = im.sel(cycle=ref_cycle, channel=ref_channel)\n",
    "    cycles_ = cycles[cycles != ref_cycle]\n",
    "    n_cy = len(cycles)\n",
    "\n",
    "    shift = [[ref_cycle, ref_cycle, ref_cycle]]\n",
    "    for cy in cycles_:\n",
    "        im_cy = im.sel(cycle=cy, channel=ref_channel)\n",
    "        start = time.time()\n",
    "        detected_shift = phase_cross_correlation(im_ref,im_cy)[0]\n",
    "        shift.append([cy]+list(detected_shift))\n",
    "        stop = time.time()\n",
    "        print(stop-start,cy, detected_shift)\n",
    "\n",
    "    shift = np.array(shift, dtype='int8')\n",
    "    np.savetxt(im.name+'_shift.txt',shift)\n",
    "\n",
    "    return shift\n",
    "\n",
    "def register_across_cycles(im, shift_path=None):\n",
    "    \"\"\"Register images across cycles according to shift input.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rows = len(im.row)\n",
    "    cols = len(im.col)\n",
    "    tiles = int(cols/im.chunks[-1][0])\n",
    "\n",
    "    if shift_path is None:\n",
    "        shift_path = im.name+'_shift.txt'\n",
    "    assert exists(shift_path)\n",
    "    shift = np.loadtxt(shift_path,dtype='int8')\n",
    "\n",
    "    ref_cycle = shift[0,0]\n",
    "    cycles_ = shift[1:,0]\n",
    "    shift = shift[1:,1:]\n",
    "\n",
    "    # adjust for global pixel shifts\n",
    "    max_row = int(np.max(shift[shift[:,0]>=0,0], initial=0))\n",
    "    min_row = int(abs(np.min(shift[shift[:,0]<=0,0], initial=0)))\n",
    "    max_col = int(np.max(shift[shift[:,1]>=0,1], initial=0))\n",
    "    min_col = int(abs(np.min(shift[shift[:,1]<=0,1], initial=0)))\n",
    "    row_del = int(max_row+min_row)\n",
    "    col_del = int(max_col+min_col)\n",
    "\n",
    "    # adjust reference image\n",
    "    row_slice = slice(max_row, rows-min_row)\n",
    "    col_slice = slice(max_col, cols-min_col)\n",
    "    im_list = [im.sel(cycle=ref_cycle,row=row_slice, col=col_slice)]\n",
    "\n",
    "    # adjust offset cycle images\n",
    "    for i, cy in enumerate(cycles_):\n",
    "        if shift[i,0] >= 0:\n",
    "            top_row = max_row-shift[i,0]\n",
    "            bot_row = rows-(row_del-top_row)\n",
    "        else:\n",
    "            bot_row = min_row+shift[i,0]\n",
    "            top_row = row_del-bot_row\n",
    "            bot_row = rows-bot_row\n",
    "        row_slice = slice(top_row,bot_row)\n",
    "\n",
    "        if shift[i,1] >= 0:\n",
    "            l_col = max_col - shift[i,1]\n",
    "            r_col = cols-(col_del - l_col)\n",
    "        else:\n",
    "            r_col = min_col+shift[i,1]\n",
    "            l_col = col_del-r_col\n",
    "            r_col = cols-r_col\n",
    "        col_slice = slice(l_col, r_col)\n",
    "\n",
    "        im_list.append(im.sel(cycle=cy, row=row_slice, col=col_slice))\n",
    "\n",
    "    # Concatenate shifted images together\n",
    "    shifted = xr.concat(im_list, dim='cycle')\n",
    "\n",
    "    # Rechunk so chunks are regular\n",
    "    dims = {'channel':1,'cycle':1,'obj_step':1,'row':rows,'col':int(cols/tiles)}\n",
    "    chunk_shape = {}\n",
    "    for d in shifted.dims:\n",
    "        chunk_shape[d] = dims[d]\n",
    "    shifted = shifted.chunk(chunk_shape)\n",
    "\n",
    "    return shifted\n",
    "\n",
    "def reshape_af(im, af_ch=610, af_cy=0):\n",
    "    \"\"\"Append autofluorescence image to each cycles of im.\"\"\"\n",
    "\n",
    "    # Reorganize images so autofluorescence image is in each cycle\n",
    "    channels = im.channel.values\n",
    "    cycles = im.cycle.values\n",
    "    af_im = im.sel(channel=af_ch, cycle=af_cy)\n",
    "\n",
    "    af_stack = []\n",
    "    for cyi in cycles:\n",
    "        if cyi != af_cy:\n",
    "            af_im = af_im.assign_coords({'channel':-af_ch,'cycle':cyi})\n",
    "            af_stack.append(af_im)\n",
    "\n",
    "    af_stack = xr.concat(af_stack,dim='cycle')\n",
    "    im_stack = im.sel(cycle=cycles[cycles != af_cy])\n",
    "    reshaped = xr.concat([af_stack, im_stack], dim='channel').transpose('channel',...)\n",
    "    reshaped = reshaped.rename('af_'+im.name)\n",
    "\n",
    "    return reshaped\n",
    "\n",
    "def one_z_plane(im):\n",
    "    \"\"\"Return only middle plane of image.\"\"\"\n",
    "\n",
    "    if 'obj_step' in im.dims:\n",
    "        # Pick middle obj_step to get unmixing matrix\n",
    "        if len(im.obj_step) % 2 == 0:\n",
    "            mid_obj_step = np.median(im.obj_step[:-1]).astype('int32')\n",
    "        else:\n",
    "            mid_obj_step = np.median(im.obj_step).astype('int32')\n",
    "        im = im.sel(obj_step=mid_obj_step)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def segment(image, client=None, min_radius = 5, sigma=1, bg_dev =3):\n",
    "    \"\"\"Segment signal in image using watershed.\n",
    "\n",
    "        Regions labeled 1 is background, labels > 1 are objects with signal.\n",
    "\n",
    "        Parameters:\n",
    "            image (dask array): Image to segment\n",
    "            client (dask client): Scheduler for workers\n",
    "            min_radius (int): Radius used for binary closing\n",
    "            sigma (int/float): Sigma used for gaussian filter\n",
    "            bg_dev (int/float): Background deviation for pixel signal threshold\n",
    "\n",
    "        Returns:\n",
    "            (dask array, int): Watersheded image and the number of labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    chunksize = image.chunksize\n",
    "\n",
    "    bg_px = da.mean(image, axis=None)\n",
    "    bg_sigma = da.std(image, axis=None)\n",
    "    px_cutoff = bg_px+bg_sigma*bg_dev\n",
    "    px_cutoff = px_cutoff.compute()\n",
    "    print('pixel cutoff value:', px_cutoff)\n",
    "\n",
    "    # Filter image\n",
    "    print('Gaussian filter with sigma =', sigma)\n",
    "    filtered_image = gaussian_filter(image, sigma=sigma).astype('int16')\n",
    "    if client is not None:\n",
    "        filtered_image = client.persist(filtered_image)\n",
    "        wait(filtered_image)\n",
    "\n",
    "    # Get local maxima of entire image\n",
    "    print('Finding local maxima of entire image')\n",
    "#     marker_mask = apply_parallel(local_maxima, filtered_image, chunks=chunksize, compute=False,\n",
    "#                                  extra_arguments=(), extra_keywords={'connectivity':2}, dtype='bool')\n",
    "    marker_mask = da.map_blocks(local_maxima, filtered_image, dtype='bool', meta='np.ndarray', connectivity=2)\n",
    "    markers = marker_mask.astype('int8')\n",
    "\n",
    "\n",
    "    # Filter local maxima that are only above pixel threshold\n",
    "    print('Filtering local maxima: threshold =', px_cutoff)\n",
    "    cell_mask = filtered_image >= px_cutoff\n",
    "\n",
    "    # Euclidean distance transform\n",
    "    print('Euclidean distance transform')\n",
    "    #edt_image = distance_transform_edt(cell_mask).astype('int8')\n",
    "    #edt_image = da.from_array(edt_image,chunks=chunksize)\n",
    "    edt_image = da.map_overlap(distance_transform_edt, cell_mask, depth = [0,100], meta='np.ndarray', dtype='uint8')\n",
    "    edt_image = gaussian_filter(edt_image, sigma=sigma).astype('uint8')\n",
    "\n",
    "    # Find seed points for cells\n",
    "    print('Find cell markers')\n",
    "#     cell_markers = apply_parallel(local_maxima, edt_image, chunks=chunksize, compute=False,\n",
    "#                                   extra_arguments=(), extra_keywords={'connectivity':2}, dtype='bool')\n",
    "    cell_markers = da.map_blocks(local_maxima, edt_image, dtype='int8', meta='np.ndarray', connectivity=2)\n",
    "    cell_markers = binary_closing(cell_markers, structure=disk(min_radius))\n",
    "\n",
    "    # Label seed points\n",
    "    print('Label cells')\n",
    "    cell_labels, n_cells = label(cell_mask)\n",
    "    n_cells = n_cells.compute()\n",
    "    print('Found', n_cells, 'cell markers')\n",
    "    cell_labels = cell_labels+1\n",
    "\n",
    "\n",
    "    # Markers for watershed, 1 is background, > 1 is cells\n",
    "    print('Set markers for watershed')\n",
    "    markers = da.where(cell_mask == True, cell_labels, markers)\n",
    "\n",
    "    watershed_image = dask_watershed(-filtered_image, markers, client)\n",
    "\n",
    "    return watershed_image, n_cells\n",
    "\n",
    "def dask_watershed(image, markers, client, **kwargs):\n",
    "    \"\"\"Dask wrapper around watershed\n",
    "\n",
    "        There will be edge effects around chunks\n",
    "\n",
    "        Parameters:\n",
    "            image (dask array): Image to segment\n",
    "            markers (dask array: Seeds for watershedding\n",
    "            client (dask client): Scheduler for workers\n",
    "\n",
    "        Returns:\n",
    "            (dask array): Watersheded image and the number of labels\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(image.shape) == 2\n",
    "    nrows, ncols = image.shape\n",
    "    nchunks = len(image.chunks[-1])\n",
    "    chunksize = image.chunksize[1]\n",
    "\n",
    "    chunk_list = []\n",
    "    for i in range(nchunks):\n",
    "        print('watershedding chunk',i+1,'of',nchunks)\n",
    "        if i != nchunks-1:\n",
    "            cols =  slice(i*chunksize,(i+1)*chunksize)\n",
    "        else:\n",
    "            cols =  slice(i*chunksize,ncols)\n",
    "        im = image[:,cols]\n",
    "        #client.persist(im)\n",
    "        marks = markers[:,cols]\n",
    "        #client.persist(marks)\n",
    "        im = im.compute()\n",
    "        marks = marks.compute()\n",
    "        ws_im = client.submit(watershed, im, markers = marks, connectivity=1)\n",
    "        chunk_list.append(da.from_delayed(ws_im, shape=im.shape, meta=np.array((),dtype=np.int32)))\n",
    "\n",
    "    print('Waiting for chunks to finish computing')\n",
    "    wait(chunk_list)\n",
    "    watershed_im = da.concatenate(chunk_list,axis=1)\n",
    "    client.persist(watershed_im)\n",
    "\n",
    "    return watershed_im\n",
    "\n",
    "def save_zarr(im, save_path, show_progress = True, name=None):\n",
    "    \"\"\"Save all sections in a zipped zarr store.\n",
    "\n",
    "       Note that coordinates for unused dimensions are not saved.\n",
    "\n",
    "       **Parameters:**\n",
    "        - save_path (path): directory to save store\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not path.isdir(save_path):\n",
    "        mkdir(save_path)\n",
    "\n",
    "    if name is None:\n",
    "        save_name = path.join(save_path,im.name+'.zarr')\n",
    "    else:\n",
    "        save_name = path.join(save_path,str(name)+'.zarr')\n",
    "\n",
    "    # Remove coordinate for unused dimensions\n",
    "    for c in im.coords.keys():\n",
    "        if c not in im.dims:\n",
    "            im = im.reset_coords(names=c, drop=True)\n",
    "\n",
    "    # if show_progress:\n",
    "    #     with ProgressBar() as pbar:\n",
    "    #         im.to_dataset().to_zarr(save_name, compute = False)\n",
    "    # else:\n",
    "    future = im.to_dataset().to_zarr(save_name, compute=False)\n",
    "\n",
    "\n",
    "    # save attributes\n",
    "    f = open(path.join(save_path, im.name+'.attrs'),\"w\")\n",
    "    for key, val in im.attrs.items():\n",
    "        f.write(str(key)+' '+str(val)+'\\n')\n",
    "    f.close()\n",
    "\n",
    "    return future\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatial",
   "language": "python",
   "name": "spatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
